{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание к уроку 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.1.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.10.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "#dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "#                                    untar=True, cache_dir='.',\n",
    "#                                    cache_subdir='')\n",
    "\n",
    "#dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "#train_dir = os.path.join(dataset_dir, 'train')\n",
    "#os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "reviews_train = []\n",
    "labels_train=[]\n",
    "for file_name in glob.glob(\"aclImdb/train/pos/*.txt\"):\n",
    "    with open(file_name, 'r', encoding='utf8') as f:\n",
    "        reviews_train.append([f.read().strip(),1])\n",
    "        labels_train.append(1)\n",
    "for file_name in glob.glob(\"aclImdb/train/neg/*.txt\"):\n",
    "    with open(file_name, 'r', encoding='utf8') as f:\n",
    "        reviews_train.append([f.read().strip(),0])\n",
    "        labels_train.append(0)\n",
    "\n",
    "labels_test=[]\n",
    "reviews_test = []\n",
    "for file_name in glob.glob(\"aclImdb/test/pos/*.txt\"):\n",
    "    with open(file_name, 'r', encoding='utf8') as f:\n",
    "        reviews_test.append([f.read().strip(),1])\n",
    "        labels_test.append(1)\n",
    "for file_name in glob.glob(\"aclImdb/test/neg/*.txt\"):\n",
    "    with open(file_name, 'r', encoding='utf8') as f:\n",
    "        reviews_test.append([f.read().strip(),0])\n",
    "        labels_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(reviews_train,columns=['text','label'])\n",
    "test=pd.DataFrame(reviews_test,columns=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train['text'],train['label'])\n",
    "xtrain_count =  count_vect.transform(train['text'])\n",
    "xtest_count =  count_vect.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.86836\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train['label'], xtest_count,test['label'])\n",
    "print(\"LR, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train['text'],train['label'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train['text'])\n",
    "xvalid_tfidf =  tfidf_vect.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Tfid Vectors:  0.8854\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train['label'], xvalid_tfidf,test['label'])\n",
    "print(\"LR, Tfid Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Обучить вашу архитектуре сети возможно туже что и была на занятии проверить\n",
    "•\tвзять предобученный эмбединг(к примеру word2vec) и загрузить в слой Embedding\n",
    "•\tвзять слой Embedding без предобученных весов\n",
    "Сравнить все подходы в том числе и полносвязанную сеть что лучше отработало\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "embedding_dim = 16\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 145)               2465      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 146       \n",
      "=================================================================\n",
      "Total params: 162,627\n",
      "Trainable params: 162,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim,),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(145),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "raw_train = tf.data.Dataset.from_tensor_slices((train['text'],train['label']))\n",
    "raw_test = tf.data.Dataset.from_tensor_slices((test['text'],test['label']))\n",
    "\n",
    "raw_train_ds = raw_train.batch(batch_size)\n",
    "raw_test_ds=raw_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps, validate for 782 steps\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.5423 - binary_accuracy: 0.7746 - val_loss: 2.3691 - val_binary_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.3583 - binary_accuracy: 0.8643 - val_loss: 2.2501 - val_binary_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3844 - binary_accuracy: 0.8434 - val_loss: 2.3562 - val_binary_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3758 - binary_accuracy: 0.8455 - val_loss: 2.4072 - val_binary_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.3855 - binary_accuracy: 0.8459 - val_loss: 2.1964 - val_binary_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3924 - binary_accuracy: 0.8330 - val_loss: 2.2465 - val_binary_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4039 - binary_accuracy: 0.8278 - val_loss: 2.1553 - val_binary_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4208 - binary_accuracy: 0.8075 - val_loss: 2.2903 - val_binary_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.4100 - binary_accuracy: 0.8189 - val_loss: 2.2572 - val_binary_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.4059 - binary_accuracy: 0.8030 - val_loss: 2.4868 - val_binary_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4870 - binary_accuracy: 0.5000\n",
      "Loss:  2.4869581636117624\n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(train_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " взять предобученный эмбединг(к примеру word2vec) и загрузить в слой Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "embedding_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 145)               3045      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 146       \n",
      "=================================================================\n",
      "Total params: 403,211\n",
      "Trainable params: 403,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(tf.keras.layers.Dense(145, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps, validate for 782 steps\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 18s 24ms/step - loss: 0.1954 - accuracy: 0.9717 - val_loss: 6.3246 - val_accuracy: 0.5002\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2908 - accuracy: 0.9458 - val_loss: 3.8305 - val_accuracy: 0.5020\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2077 - accuracy: 0.9469 - val_loss: 2.8921 - val_accuracy: 0.5016\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.1740 - accuracy: 0.9480 - val_loss: 2.8517 - val_accuracy: 0.5027\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.1500 - accuracy: 0.9541 - val_loss: 2.4507 - val_accuracy: 0.5048\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.1292 - accuracy: 0.9582 - val_loss: 2.4041 - val_accuracy: 0.5068\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.1125 - accuracy: 0.9631 - val_loss: 2.2087 - val_accuracy: 0.5183\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.0953 - accuracy: 0.9689 - val_loss: 2.2168 - val_accuracy: 0.5223\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.0787 - accuracy: 0.9745 - val_loss: 2.1513 - val_accuracy: 0.5360\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.0646 - accuracy: 0.9799 - val_loss: 2.2536 - val_accuracy: 0.5428\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    raw_train_ds,\n",
    "    validation_data=raw_test_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 10ms/step - loss: 2.2536 - accuracy: 0.5428\n",
      "Loss:  2.253603813271826\n",
      "Accuracy:  0.5428\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(raw_test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть 2 дополню чуть познее "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
